# Apache Spark

An [Apache Spark](http://spark.apache.org/) container image for development and testing purposes. The image is meant to be used for creating an standalone cluster with one or several workers.

- [`alpine` (Dockerfile)](https://github.com/SingularitiesCR/spark-docker/blob/alpine/Dockerfile)
- [`1.5` (Dockerfile)](https://github.com/SingularitiesCR/spark-docker/blob/1.5/Dockerfile)
- [`1.5-alpine` (Dockerfile)](https://github.com/SingularitiesCR/spark-docker/blob/1.5-alpine/Dockerfile)
- [`1.6` (Dockerfile)](https://github.com/SingularitiesCR/spark-docker/blob/1.6/Dockerfile)
- [`1.6-alpine` (Dockerfile)](https://github.com/SingularitiesCR/spark-docker/blob/1.6-alpine/Dockerfile)

## Entrypoint

The default entry point is set to run with the command `master` or `worker`.

The `master` command will create a Spark master with an HDFS name node. It is advised to set an explicit hostname when running a master since HDFS might not support the hostnames generated by Docker.

The `worker` command will create a Spark worker with an HDFS data node. In order for the worker to find the master the hostname or IP Address must be specified as part of the command.

## Creating a Cluster with Docker Compose

The easiest way to create a standalone cluster with this image is by using [Docker Compose](https://docs.docker.com/compose). The following snippet can be used as a `docker-compose.yml` for a simple cluster:

```YAML
version: "2"

services:
  sparkmaster:
    image: singularities/spark
    command: master
    hostname: sparkmaster
    ports:
      - "6066:6066"
      - "7070:7070"
      - "8080:8080"
      - "50070:50070"
  sparkworker:
    image: singularities/spark
    command: worker sparkmaster
    environment:
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 2g
    links:
      - sparkmaster
```

All Spark and HDFS ports are exposed by the image. In the example compose file we only map the Spark submit port and the ports for the web clients.

If you wish to increase the number of workers the `sparkworker` service can be scale using the `scale` command like follows:

```sh
docker-compose scale sparkworker=2
```

The workers will automatically register themselves with the master node.

## Versions

This version of the container image is uses the following components:

- Java: `OpenJDK 8`
- Hadoop: `2.7.2 `
- Spark: `1.5.2`
